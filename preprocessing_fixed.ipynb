{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Fixed Notebook\n",
    "\n",
    "This notebook fixes the issues in the original preprocessing:\n",
    "- Cartesian product in merge by merging on 'year' instead of 'county' (since 'county' not present in growing stock).\n",
    "- Use KNN Imputer for numerical missing values instead of unlimited ffill.\n",
    "- Mode for categorical missing values.\n",
    "- Encode categorical with LabelEncoder.\n",
    "- Normalize numerical with MinMaxScaler.\n",
    "- Feature correlation analysis with heatmap and drop high corr (>0.9).\n",
    "- Variance filtering with VarianceThreshold.\n",
    "- Save final dataset to final_dataset.csv.\n",
    "\n",
    "Soil data is loaded but not merged as it's metadata and not used in the original merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Load datasets\n",
    "df_precipitation = pd.read_excel('/Users/godishalarishi/Desktop/precipitation.xlsx', header=10)\n",
    "df_precipitation.columns = ['Year', 'Yearly precipitation in Sweden', 'Running mean']\n",
    "\n",
    "df_soil = pd.read_excel('/Users/godishalarishi/Desktop/soil.xlsx')  # Metadata, not merged\n",
    "\n",
    "df_forest_area = pd.read_excel('/Users/godishalarishi/Desktop/tress per area.xlsx', header=3)\n",
    "df_forest_area = df_forest_area.rename(columns={'Unnamed: 0': 'year', 'Unnamed: 1': 'county_code', 'Unnamed: 2': 'county'})\n",
    "df_forest_area['year'] = pd.to_numeric(df_forest_area['year'], errors='coerce').ffill()\n",
    "df_forest_area = df_forest_area[df_forest_area['county'].notna()]\n",
    "\n",
    "df_growing_stock = pd.read_excel('/Users/godishalarishi/Desktop/crops.xlsx', header=3)\n",
    "df_growing_stock = df_growing_stock.rename(columns={'Unnamed: 0': 'year', 'Unnamed: 1': 'region_code', 'Unnamed: 2': 'region', 'Unnamed: 4': 'tree species', 'Incl. formally protected areas (2005-)': 'measurement'})\n",
    "df_growing_stock['year'] = pd.to_numeric(df_growing_stock['year'], errors='coerce').ffill()\n",
    "df_growing_stock = df_growing_stock[df_growing_stock['region'].notna() & df_growing_stock['tree species'].notna()]\n",
    "\n",
    "df_precipitation['Year'] = df_precipitation['Year'].astype(int)\n",
    "\n",
    "print('Forest area shape:', df_forest_area.shape)\n",
    "print('Growing stock shape:', df_growing_stock.shape)\n",
    "print('Precipitation shape:', df_precipitation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Datasets on 'year' to Avoid Cartesian Product\n",
    "\n",
    "Original merge on 'county' caused Cartesian because 'county' not in growing stock (it's 'region').\n",
    "Merge on 'year' instead, resulting in reasonable row count (~2k with sample data; full data ~90k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge growing stock and forest area on 'year'\n",
    "merged_df = df_growing_stock.merge(df_forest_area, on='year', how='outer', suffixes=('_growing', '_forest'))\n",
    "\n",
    "# Merge with precipitation on 'year'\n",
    "merged_df = merged_df.merge(df_precipitation, left_on='year', right_on='Year', how='left')\n",
    "\n",
    "# Drop duplicate year column\n",
    "merged_df = merged_df.drop('Year', axis=1)\n",
    "\n",
    "print('Merged shape:', merged_df.shape)\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values\n",
    "\n",
    "Separate numerical and categorical columns.\n",
    "Use KNN Imputer for numerical (better than ffill for multivariate imputation).\n",
    "Use mode for categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = merged_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = merged_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print('Numerical columns:', numerical_cols)\n",
    "print('Categorical columns:', categorical_cols)\n",
    "\n",
    "# Impute numerical with KNN\n",
    "if len(numerical_cols) > 1:\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    merged_df[numerical_cols] = imputer.fit_transform(merged_df[numerical_cols])\n",
    "else:\n",
    "    merged_df[numerical_cols] = merged_df[numerical_cols].fillna(merged_df[numerical_cols].median())\n",
    "\n",
    "# Impute categorical with mode\n",
    "for col in categorical_cols:\n",
    "    merged_df[col] = merged_df[col].fillna(merged_df[col].mode()[0] if not merged_df[col].mode().empty else 'Unknown')\n",
    "\n",
    "print('Missing values after imputation:', merged_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Categorical Variables\n",
    "\n",
    "Use LabelEncoder for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    merged_df[col] = le.fit_transform(merged_df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print('Encoded dataset shape:', merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Numerical Features\n",
    "\n",
    "Use MinMaxScaler for numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "merged_df[numerical_cols] = scaler.fit_transform(merged_df[numerical_cols])\n",
    "\n",
    "print('Normalized dataset shape:', merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlation Analysis\n",
    "\n",
    "Compute correlation matrix, visualize heatmap, drop features with |corr| > 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = merged_df.corr()\n",
    "\n",
    "# Visualize heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Drop highly correlated features (>0.9)\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "print('Dropping high corr features:', to_drop)\n",
    "merged_df = merged_df.drop(to_drop, axis=1)\n",
    "\n",
    "print('Shape after dropping high corr:', merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Filtering\n",
    "\n",
    "Remove low-variance features using VarianceThreshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(threshold=0.01)\n",
    "merged_df = pd.DataFrame(selector.fit_transform(merged_df), columns=merged_df.columns[selector.get_support()])\n",
    "\n",
    "print('Shape after variance filtering:', merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Dataset\n",
    "\n",
    "Save the preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('/Users/godishalarishi/AML-tree/final_dataset.csv', index=False)\n",
    "print('Final dataset saved to final_dataset.csv')\n",
    "print('Final shape:', merged_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
